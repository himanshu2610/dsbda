{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fffa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ankush mehta\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ankush mehta\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\ankush mehta\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ankush mehta\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ankush mehta\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ankush mehta\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57a8426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ankush\n",
      "[nltk_data]     Mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ankush Mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Ankush\n",
      "[nltk_data]     Mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ankush\n",
      "[nltk_data]     Mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ankush\n",
      "[nltk_data]     Mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Ankush\n",
      "[nltk_data]     Mehta\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d15c921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone! This is a text analysis assignment. We will be using preprocessing methods like Tokenization, POS tagging , stop words removal, stemming and lemmatization.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('text_doc.txt').read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a28c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_sent = nltk.sent_tokenize(text)\n",
    "tokens_words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58a9f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone!', 'This is a text analysis assignment.', 'We will be using preprocessing methods like Tokenization, POS tagging , stop words removal, stemming and lemmatization.']\n",
      "['Hello', 'everyone', '!', 'This', 'is', 'a', 'text', 'analysis', 'assignment', '.', 'We', 'will', 'be', 'using', 'preprocessing', 'methods', 'like', 'Tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_sent)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9986c3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyone', '!', 'this', 'is', 'a', 'text', 'analysis', 'assignment', '.', 'we', 'will', 'be', 'using', 'preprocessing', 'methods', 'like', 'tokenization', ',', 'pos', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_words_lower = []\n",
    "for word in tokens_words:\n",
    "    tokens_words_lower.append(word.lower())\n",
    "print(tokens_words_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a773b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens_words,tagset = 'universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3982548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NOUN'), ('everyone', 'NOUN'), ('!', '.'), ('This', 'DET'), ('is', 'VERB'), ('a', 'DET'), ('text', 'ADJ'), ('analysis', 'NOUN'), ('assignment', 'NOUN'), ('.', '.'), ('We', 'PRON'), ('will', 'VERB'), ('be', 'VERB'), ('using', 'VERB'), ('preprocessing', 'VERB'), ('methods', 'NOUN'), ('like', 'ADP'), ('Tokenization', 'NOUN'), (',', '.'), ('POS', 'NOUN'), ('tagging', 'NOUN'), (',', '.'), ('stop', 'VERB'), ('words', 'NOUN'), ('removal', 'ADJ'), (',', '.'), ('stemming', 'VERB'), ('and', 'CONJ'), ('lemmatization', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32f9ad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76c968ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffa283cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyone', '!', 'text', 'analysis', 'assignment', '.', 'using', 'preprocessing', 'methods', 'like', 'tokenization', ',', 'pos', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "clean_words = []\n",
    "for word in tokens_words_lower:\n",
    "    if word not in stop_words:\n",
    "        clean_words.append(word)\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0073e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eac2bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyon', '!', 'thi', 'is', 'a', 'text', 'analysi', 'assign', '.', 'we', 'will', 'be', 'use', 'preprocess', 'method', 'like', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', 'and', 'lemmat', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stem_words = []\n",
    "for word in tokens_words_lower:\n",
    "    stem_words.append(stemmer.stem(word))\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3bcceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c958f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "958f27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'everyone', '!', 'this', 'is', 'a', 'text', 'analysis', 'assignment', '.', 'we', 'will', 'be', 'using', 'preprocessing', 'method', 'like', 'tokenization', ',', 'po', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "lem_words = []\n",
    "for word in tokens_words_lower:\n",
    "    lem_words.append(lemmatizer.lemmatize(word))\n",
    "print(lem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d87f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tokens_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ff84e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone!',\n",
       " 'This is a text analysis assignment.',\n",
       " 'We will be using preprocessing methods like Tokenization, POS tagging , stop words removal, stemming and lemmatization.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebf6c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cor = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "434f9ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbf58d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "714c309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus:  25\n",
      "Words in the corpus:  {'text', 'words', 'preprocessing', 'assignment.', ',', 'This', 'lemmatization.', 'like', 'and', 'POS', 'using', 'be', 'will', 'everyone!', 'We', 'Hello', 'is', 'analysis', 'stop', 'removal,', 'tagging', 'a', 'stemming', 'methods', 'Tokenization,'}\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "print(\"Number of words in the corpus: \", len(words_set))\n",
    "print(\"Words in the corpus: \",words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36f2ad7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>assignment.</th>\n",
       "      <th>,</th>\n",
       "      <th>This</th>\n",
       "      <th>lemmatization.</th>\n",
       "      <th>like</th>\n",
       "      <th>and</th>\n",
       "      <th>POS</th>\n",
       "      <th>...</th>\n",
       "      <th>Hello</th>\n",
       "      <th>is</th>\n",
       "      <th>analysis</th>\n",
       "      <th>stop</th>\n",
       "      <th>removal,</th>\n",
       "      <th>tagging</th>\n",
       "      <th>a</th>\n",
       "      <th>stemming</th>\n",
       "      <th>methods</th>\n",
       "      <th>Tokenization,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text     words  preprocessing  assignment.         ,      This  \\\n",
       "0  0.000000  0.000000       0.000000     0.000000  0.000000  0.000000   \n",
       "1  0.166667  0.000000       0.000000     0.166667  0.000000  0.166667   \n",
       "2  0.000000  0.058824       0.058824     0.000000  0.058824  0.000000   \n",
       "\n",
       "   lemmatization.      like       and       POS  ...  Hello        is  \\\n",
       "0        0.000000  0.000000  0.000000  0.000000  ...    0.5  0.000000   \n",
       "1        0.000000  0.000000  0.000000  0.000000  ...    0.0  0.166667   \n",
       "2        0.058824  0.058824  0.058824  0.058824  ...    0.0  0.000000   \n",
       "\n",
       "   analysis      stop  removal,   tagging         a  stemming   methods  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.166667  0.000000  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
       "2  0.000000  0.058824  0.058824  0.058824  0.000000  0.058824  0.058824   \n",
       "\n",
       "   Tokenization,  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       0.058824  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_cor,n_words_set)),columns = list(words_set))\n",
    "for i in range(n_cor):\n",
    "    words = corpus[i].split(\" \")\n",
    "    for word in words:\n",
    "        df_tf[word][i] = df_tf[word][i] + (1/len(words))\n",
    "df_tf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79e99226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF of: \n",
      "text:0.47712125471966244\n",
      "words:0.47712125471966244\n",
      "preprocessing:0.47712125471966244\n",
      "assignment.:0.47712125471966244\n",
      ",:0.47712125471966244\n",
      "This:0.47712125471966244\n",
      "lemmatization.:0.47712125471966244\n",
      "like:0.47712125471966244\n",
      "and:0.47712125471966244\n",
      "POS:0.47712125471966244\n",
      "using:0.47712125471966244\n",
      "be:0.47712125471966244\n",
      "will:0.47712125471966244\n",
      "everyone!:0.47712125471966244\n",
      "We:0.47712125471966244\n",
      "Hello:0.47712125471966244\n",
      "is:0.47712125471966244\n",
      "analysis:0.47712125471966244\n",
      "stop:0.47712125471966244\n",
      "removal,:0.47712125471966244\n",
      "tagging:0.47712125471966244\n",
      "a:0.47712125471966244\n",
      "stemming:0.47712125471966244\n",
      "methods:0.47712125471966244\n",
      "Tokenization,:0.47712125471966244\n"
     ]
    }
   ],
   "source": [
    "print(\"IDF of: \")\n",
    "idf = {}\n",
    "for word in words_set:\n",
    "    k = 0\n",
    "    for i in range(n_cor):\n",
    "        words = corpus[i].split(\" \")\n",
    "        if word in words:\n",
    "            k+=1\n",
    "    idf[word] = np.log10(n_cor/k)\n",
    "    print(\"{}:{}\".format(word,idf[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd5ba44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>assignment.</th>\n",
       "      <th>,</th>\n",
       "      <th>This</th>\n",
       "      <th>lemmatization.</th>\n",
       "      <th>like</th>\n",
       "      <th>and</th>\n",
       "      <th>POS</th>\n",
       "      <th>...</th>\n",
       "      <th>Hello</th>\n",
       "      <th>is</th>\n",
       "      <th>analysis</th>\n",
       "      <th>stop</th>\n",
       "      <th>removal,</th>\n",
       "      <th>tagging</th>\n",
       "      <th>a</th>\n",
       "      <th>stemming</th>\n",
       "      <th>methods</th>\n",
       "      <th>Tokenization,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238561</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>0.028066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      text     words  preprocessing  assignment.         ,     This  \\\n",
       "0  0.00000  0.000000       0.000000      0.00000  0.000000  0.00000   \n",
       "1  0.07952  0.000000       0.000000      0.07952  0.000000  0.07952   \n",
       "2  0.00000  0.028066       0.028066      0.00000  0.028066  0.00000   \n",
       "\n",
       "   lemmatization.      like       and       POS  ...     Hello       is  \\\n",
       "0        0.000000  0.000000  0.000000  0.000000  ...  0.238561  0.00000   \n",
       "1        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.07952   \n",
       "2        0.028066  0.028066  0.028066  0.028066  ...  0.000000  0.00000   \n",
       "\n",
       "   analysis      stop  removal,   tagging        a  stemming   methods  \\\n",
       "0   0.00000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "1   0.07952  0.000000  0.000000  0.000000  0.07952  0.000000  0.000000   \n",
       "2   0.00000  0.028066  0.028066  0.028066  0.00000  0.028066  0.028066   \n",
       "\n",
       "   Tokenization,  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       0.028066  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf = df_tf.copy()\n",
    "for word in words_set:\n",
    "    for i in range(n_cor):\n",
    "        df_tf_idf[word][i] = df_tf[word][i] * idf[word]\n",
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cfcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
